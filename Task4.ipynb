{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import  request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.gutenberg.org/files/98/98-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=request.urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens=word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'A', 'Tale', 'of', 'Two', 'Cities', ',', 'by', 'Charles', 'Dickens', 'This', 'eBook', 'is', 'for', 'the', 'use']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we would get words like /ufeffThe -> then we have to use regular expression to get rid of all this, even when we request a html page\n",
    "# we've to take care of tags\n",
    "# Beautiful_soups -> package help to extract(scrap) content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BeautifulSoup\n",
    "#Preprocessing - RegEx to clean any html tags or unnecessary char sequences\n",
    "#Pos Tagging\n",
    "#!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joyou'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#types of stemmer - lancaster, potter, snowball, regEx stemmer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem('joyous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cacti'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('cacti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joyou'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "lancaster.stem('joyous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cact'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster.stem('cacti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "regExp = RegexpStemmer('ing')\n",
    "regExp.stem('singing')\n",
    "#singing -> s + ing + ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#walking -> walk + ing\n",
    "regExp.stem('walking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sing'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regExp to work it for singing\n",
    "regExp= RegexpStemmer('ing$')\n",
    "regExp.stem('singing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not work for 'sing' -> ing$\n",
    "regExp.stem('sing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mang'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer('french')\n",
    "snowball.stem('manges')\n",
    "#manges is a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arbeit'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball=SnowballStemmer('german')\n",
    "snowball.stem('arbeiten')\n",
    "#arbeiten (work) -> arbeit is correct stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffthe',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'a',\n",
       " 'tal',\n",
       " 'of',\n",
       " 'two',\n",
       " 'citi',\n",
       " ',',\n",
       " 'by',\n",
       " 'charl',\n",
       " 'dick',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemTokens=[]\n",
    "for i in range(20):\n",
    "    stemTokens.append(snowball.stem(tokens[i]))\n",
    "stemTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'of',\n",
       " 'times,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'of',\n",
       " 'times,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'ag',\n",
       " 'of',\n",
       " 'wisdom,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'ag',\n",
       " 'of',\n",
       " 'foolishness,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'epoch',\n",
       " 'of',\n",
       " 'belief,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'epoch',\n",
       " 'of',\n",
       " 'incredulity,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'season',\n",
       " 'of',\n",
       " 'light,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'season',\n",
       " 'of',\n",
       " 'darkness,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'spring',\n",
       " 'of',\n",
       " 'hope,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'wint',\n",
       " 'of',\n",
       " 'despair,',\n",
       " 'we',\n",
       " 'had',\n",
       " 'everyth',\n",
       " 'bef',\n",
       " 'us,',\n",
       " 'we',\n",
       " 'had',\n",
       " 'noth',\n",
       " 'bef',\n",
       " 'us,',\n",
       " 'we',\n",
       " 'wer',\n",
       " 'al',\n",
       " 'going',\n",
       " 'direct',\n",
       " 'to',\n",
       " 'heaven,',\n",
       " 'we',\n",
       " 'wer',\n",
       " 'al',\n",
       " 'going',\n",
       " 'direct',\n",
       " 'the',\n",
       " 'oth',\n",
       " 'way',\n",
       " 'in',\n",
       " 'short,',\n",
       " 'the',\n",
       " 'period',\n",
       " 'was',\n",
       " 'so',\n",
       " 'far',\n",
       " 'lik',\n",
       " 'the',\n",
       " 'pres',\n",
       " 'period,',\n",
       " 'that',\n",
       " 'som',\n",
       " 'of',\n",
       " 'it',\n",
       " 'noisiest',\n",
       " 'auth',\n",
       " 'insist',\n",
       " 'on',\n",
       " 'it',\n",
       " 'being',\n",
       " 'received,',\n",
       " 'for',\n",
       " 'good',\n",
       " 'or',\n",
       " 'for',\n",
       " 'evil,',\n",
       " 'in',\n",
       " 'the',\n",
       " 'superl',\n",
       " 'degr',\n",
       " 'of',\n",
       " 'comparison',\n",
       " 'on']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only\"\n",
    "\n",
    "stemmedTokens = [lancaster.stem(token) for token in text.split(\" \")]\n",
    "stemmedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'am'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization -> root\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma.lemmatize('am')\n",
    "#root of (am) is being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cactus'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('cacti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('mice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also give second argument, that is part of speech then the lemma will be that part of speech (pos)\n",
    "#Valid options are \"n\" for nouns,\"v\" for verbs, \"a\" for adjectives, \"r\" for adverbs and \"s\"\n",
    "lemma.lemmatize('am',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('better',pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('better',pos='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stripe'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('stripes',pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strip'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('stripes',pos='v')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20643ba39cc192cecd775d36cbcb290681c3cfe28e501fd357f2d90d955dc5c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
